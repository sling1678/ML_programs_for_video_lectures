{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forward_stepwise_subset_selection_with_cross_validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwpN3++jXMsg3THCtdRE/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sling1678/ML_programs_for_video_lectures/blob/main/forward_stepwise_subset_selection_with_cross_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Illustration of Forward-Stepwise Subset Selection for Dimensional Reduction\n",
        "\n",
        "We will use the prostate cancer data referred to in the Elements of Statistical Learning (ESL). The data has eight-dimensional predictor\n",
        "\n",
        "x_1 = log of cancer volume (lcavol); NUMERICAL\n",
        "\n",
        "x_2 = log of prostate weight (lweight); NUMERICAL\n",
        "\n",
        "x_3 = age (age); NUMERICAL\n",
        "\n",
        "x_4 = log of amount of benign prostatic hyperplasia (lbph); NUMERICAL\n",
        "\n",
        "x_5 = seminal vesicle invasion (svi); INTEGER_CATEGORICAL\n",
        "\n",
        "x_6 = log of capsular penetration (lcp); NUMERICAL\n",
        "\n",
        "x_7 = Gleason score (gleason); INTEGER_CATEGORICAL\n",
        "\n",
        "x_8 = percent of Gleason scores 4 or 5 (pgg45). NUMERICAL\n",
        "\n",
        "Fig. 3.5 in ESL shows that 3 or 4 captures most of the predictive power as judged by residual sum of squares.\n",
        "\n",
        "## Subset Selection permits using fewer predictor variables than present in the X part of the (X,y) dataset. You might say, these are most \"important\" variables in some sense. You get to define the sense in which the variable you keep are important. We will use least Residual Sum-of-Squares since we will be working here with linear regression. For other types of problems, you will use other metrics to decide what would constitute important relation between an X variable and the target y variable.\n",
        "\n",
        "## The model, defined by the values of (b,w) in y = b + wx + e, will be trained and evaluated by cross-validation. In this way of doing things, you will split the data availabe for training into K cross-validation parts and pick one of those parts for testing and the other parts for training. That is, you will traing and test your model K times. Then, you average over the results. These are already done in sklearn. So, we will not reinvent the wheels and just use the function availabel in sklearn. Here is the reference: https://scikit-learn.org/stable/modules/cross_validation.html."
      ],
      "metadata": {
        "id": "up-5Z-yQHz6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UwqMxqmKHFPW"
      },
      "outputs": [],
      "source": [
        "#IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import itertools\n",
        "\n",
        "from sklearn import linear_model # This will save time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import make_scorer # to convert metrics into scoring function\n",
        "\n",
        "from sklearn.model_selection import train_test_split # this in case we work with full data and need to set aside test data\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "#-----------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging mode?\n",
        "DEBUGGING = False"
      ],
      "metadata": {
        "id": "p2gkmOSYRQt9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Data"
      ],
      "metadata": {
        "id": "jw6m3DU7P8j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe for this project\n",
        "DATA_URL = \"https://hastie.su.domains/ElemStatLearn/datasets/prostate.data\"\n",
        "df = pd.read_csv(DATA_URL, sep='\\t') \n",
        "\n",
        "TARGET = ['lpsa']\n",
        "ALL_FEATURES = ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']\n",
        "TRAINING_SET_SELECTION = ['train']\n",
        "df = df[ALL_FEATURES + TARGET + TRAINING_SET_SELECTION] # drops fictitious columns\n",
        "if DEBUGGING:\n",
        "  df.head(3)# check"
      ],
      "metadata": {
        "id": "fJHlPy0EP3cQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove extra columns in the data \n",
        "def clean_df(df, dropcols=None):\n",
        "  if dropcols is not None:\n",
        "    for col in dropcols:\n",
        "      if col in df.columns:\n",
        "        df.drop(col, axis=1, inplace=True)\n",
        "  return df  \n",
        "\n",
        "def prepare_train_and_test_sets(df):\n",
        "  # special for this dataset; datapoints to be used in training\n",
        "  # are labeled in a separate calumn with letter 'T'\n",
        "  train_col_name, train_value=\"train\",\"T\"\n",
        "  train = df[df[train_col_name]==train_value].copy()\n",
        "  train.drop(columns=[train_col_name], axis=1, inplace=True)\n",
        "  train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  test = df[df[train_col_name]!=train_value].copy()\n",
        "  test.drop(columns=[train_col_name], axis=1, inplace=True)\n",
        "  test.reset_index(drop=True, inplace=True)\n",
        "  return train, test\n",
        "#-------------------------------------------------------\n",
        "train, test = prepare_train_and_test_sets(df)\n",
        "\n",
        "if DEBUGGING:\n",
        "  print(f\"full dataframe shape:{df.shape}\")\n",
        "  print(f\"train dataframe shape:{train.shape}\")\n",
        "  print(f\"test dataframe shape:{test.shape}\")\n",
        "  print(f\"train dataframe first two rows:\\n{train.head(2)}\")"
      ],
      "metadata": {
        "id": "crpg6-uiQC1N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing- here only normalizing the train and test datasets based on train\n",
        "def preprocess_params(train, features):\n",
        "  means = train[features].mean()\n",
        "  stds = train[features].std()\n",
        "  return means, stds\n",
        "def preprocess(train, test, features):\n",
        "  means, stds = preprocess_params(train, features)\n",
        "  train[features] = (train[features]-means)/stds\n",
        "  test[features] = (test[features]-means)/stds\n",
        "  return train, test\n",
        "#-------------------------------------------------------\n",
        "train, test = preprocess(train, test, features=ALL_FEATURES)\n",
        "if DEBUGGING:\n",
        "  print(train.describe()) # check that mean and std are properly normalized"
      ],
      "metadata": {
        "id": "lAZh59fBQOB_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training\n",
        "\n",
        "## We will have $p+1$ models, where $p$ is the dimension of the X variable."
      ],
      "metadata": {
        "id": "cP5fE-68Q9hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.function_base import average\n",
        "import sys\n",
        "NUM_FOLDS = 10\n",
        "NUM_CV_TRIALS = 30 # for std error in averages of each cv splits run\n",
        "\n",
        "def print_list(L, name=\"\", num_digits_for_rounding=2):\n",
        "  print(name+\":\")\n",
        "  sys.stdout.write(\"[\\n\")\n",
        "  for i,value in enumerate(L):\n",
        "    sys.stdout.write(str(np.round(value, num_digits_for_rounding))+\", \")\n",
        "    if (i+1)%10 == 0:\n",
        "      sys.stdout.write(\"\\n\")\n",
        "  sys.stdout.write(\"]\\n\")  \n",
        "\n",
        "def fit_and_evaluate_linear_model_cv(train, target, features=None, \n",
        "  num_folds=10, num_cv_trials=1, verbose=0):\n",
        "  results = dict() # collection of outputs\n",
        "  train = train.copy()\n",
        "  y = train[target]\n",
        "  X = train.drop(target, axis=1)\n",
        "  if features is not None:\n",
        "    X = X[features] # These are the features we will be working on in forward_stepwise algo\n",
        "  features = X.columns\n",
        "  RSS_cv_trials=[]\n",
        "  for i in range(num_cv_trials):\n",
        "    cv = KFold(n_splits=num_folds, random_state=None, shuffle=True) # each time get new split\n",
        "    RSS_list=[]\n",
        "    for train_index, val_index in cv.split(X):\n",
        "      model = linear_model.LinearRegression(fit_intercept = True)\n",
        "      X_train, y_train = X.iloc[train_index,:],  y.iloc[train_index,:]\n",
        "      X_val, y_val = X.iloc[val_index, :], y.iloc[val_index, :]\n",
        "      model.fit(X_train, y_train)\n",
        "      RSS = mean_squared_error(y_val, model.predict(X_val)) * len(X_val)\n",
        "      RSS_list.append(RSS)\n",
        "    RSS_average_this_cv = np.mean(np.array(RSS_list))\n",
        "    RSS_cv_trials.append(RSS_average_this_cv)\n",
        "\n",
        "  RSS_average = np.mean(np.array(RSS_cv_trials))\n",
        "  RSS_stderr = np.std(np.array(RSS_cv_trials))/np.sqrt(NUM_CV_TRIALS)\n",
        "\n",
        "  results[\"RSS\"] = (RSS_average, RSS_stderr)\n",
        "  results[\"model\"] = model, features, target\n",
        "  if verbose !=0:\n",
        "    print_list(RSS_cv_trials, \"RSS in different cv trials\", 3)\n",
        "    print(f\"RSS = {RSS_average:0.3f} \" + u\"\\u00B1\" + f\" {RSS_stderr:0.3f}\")\n",
        "    print(f\"Intercept = {model.intercept_[0]:0.3f}\")\n",
        "    print_list(model.coef_[0], \"Coefficients of X\", 3)\n",
        "\n",
        "  return results\n",
        "\n",
        "if DEBUGGING:\n",
        "  results = fit_and_evaluate_linear_model_cv(train, target=TARGET, features=None, \n",
        "    num_folds=NUM_FOLDS, num_cv_trials=NUM_CV_TRIALS, verbose=1) #OK\n"
      ],
      "metadata": {
        "id": "m6D55xpCQWBV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_forward_stepwise_selection(df, target, num_folds=5, num_cv_trials=1):\n",
        "  results = []\n",
        "  df = df.copy()\n",
        "  y = df[target]\n",
        "  X = df.drop(target, axis=1)\n",
        "  # for k=0, the empty set, we predict the mean\n",
        "  predicted_y = np.mean(y.values)\n",
        "  RSS_none = np.sum(np.square(y.values-predicted_y))\n",
        "  features_done=('None',)\n",
        "  results.append( (RSS_none, tuple(features_done)) )\n",
        "  features_done = []\n",
        "  features_to_explore = list(X.columns) #OK\n",
        "\n",
        "  while len(features_to_explore) != 0:\n",
        "\n",
        "    temp_results = []\n",
        "    temp_features = []\n",
        "    for idx in range(len(features_to_explore)):\n",
        "      new_feature_to_try = np.random.choice(features_to_explore)\n",
        "      if features_done is not None:\n",
        "        features_trying = features_done + [new_feature_to_try]\n",
        "      else:\n",
        "        features_trying = [new_feature_to_try]\n",
        "      RSS_and_model = fit_and_evaluate_linear_model_cv(train, target,\n",
        "        features=features_trying, num_folds=num_folds,\n",
        "        num_cv_trials=num_cv_trials, verbose=0)\n",
        "      RSS = RSS_and_model[\"RSS\"][0]\n",
        "      temp_results.append(RSS)\n",
        "      temp_features.append(new_feature_to_try)\n",
        "    \n",
        "    min_RSS = min(temp_results)\n",
        "    min_index = temp_results.index(min_RSS)\n",
        "    feature_to_add = temp_features[min_index]\n",
        "\n",
        "    features_to_explore.remove(feature_to_add)\n",
        "    if features_done is not None:\n",
        "      features_done = features_done + [feature_to_add]\n",
        "    else:\n",
        "      features_done = [feature_to_add]\n",
        " \n",
        "    results_package = (min_RSS, tuple(features_done))   \n",
        "    results.append(results_package) \n",
        "\n",
        "  return results\n",
        "\n",
        "if DEBUGGING:\n",
        "  r = do_forward_stepwise_selection(train, TARGET, NUM_FOLDS, NUM_CV_TRIALS)"
      ],
      "metadata": {
        "id": "mTmYdiatnPj2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_results_forward_stepwise(results, verbose=0):\n",
        "  \"\"\"\n",
        "  Set verbose = 0 to stop printout.\n",
        "  \"\"\"\n",
        "  RSS_list=[]\n",
        "  features_list=[]\n",
        "  for item in results:\n",
        "    RSS_list.append(item[0])\n",
        "    features_list.append(item[1])\n",
        "\n",
        "  if verbose!=0:\n",
        "    for k,features in enumerate(features_list):\n",
        "      print(f\"for k = {k}, the best features by forward procedure are {features}\")\n",
        "  \n",
        "  return RSS_list, features_list\n",
        " \n",
        "def get_best_features(train, target, num_folds=5, num_cv_trials=1, plot_rss_vs_k=True):\n",
        "  results = do_forward_stepwise_selection(train, target, num_folds, num_cv_trials)\n",
        "  rss, best_features = process_results_forward_stepwise(results)\n",
        "  idx = np.where(rss==min(rss))\n",
        " \n",
        "  if plot_rss_vs_k:\n",
        "    y1=np.array(rss[1:])\n",
        "    x1=np.arange(len(y1)) + 1\n",
        "    plt.plot(x1, y1, 'o')\n",
        "\n",
        "  return list(best_features[int(idx[0])])\n",
        "\n",
        "#------------------\n",
        "if DEBUGGING:\n",
        "  %timeit results = do_forward_stepwise_selection(train, TARGET)\n",
        "\n",
        "if DEBUGGING:\n",
        "  print(\"\\nForward-Stepwise Selection Algorithm Results:\")\n",
        "  results = do_forward_stepwise_selection(train, TARGET, NUM_FOLDS, NUM_CV_TRIALS)\n",
        "  rss, best_features = process_results_forward_stepwise(results)\n",
        "  y1=np.array(rss[1:])\n",
        "  x1=np.arange(len(y1)) + 1\n",
        "  plt.plot(x1, y1, 'o')"
      ],
      "metadata": {
        "id": "C9mO1Al13B54"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection\n",
        "## Models are indexed by number of variables that we trained on. For us they are $M_0$, $M_1$, $...$, $M_p$, with $p=8$. We found that $M_3$ is sufficient in the last run. So, we pick model $M_3$ with following variables."
      ],
      "metadata": {
        "id": "OLfLZRHb4XhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_run_regression(train, test, features_to_use, model=None, eval_metric=mean_squared_error,\n",
        "  verbose = 0):\n",
        "\n",
        "  if model is None: # try linear regression\n",
        "    model = linear_model.LinearRegression(fit_intercept = True)\n",
        "\n",
        "  X, X_test = train[features_to_use].values, test[features_to_use].values\n",
        "  y, y_test = train[TARGET].values, test[TARGET].values\n",
        "  y_ref = np.array([np.mean(y)] * len(y_test)) # use mean of y and predict on y_test\n",
        "  \n",
        "  model.fit(X, y)\n",
        "\n",
        "  mse_in = eval_metric(y, model.predict(X))   \n",
        "  mse_out = eval_metric(y_test, model.predict(X_test))  \n",
        "  mse_ref = eval_metric(y_test, y_ref)\n",
        "\n",
        "  if mse_ref == 0: # just to avoid divide by zero error\n",
        "    mse_ref = 1.0e-6\n",
        "  pct_improvement = 100*(mse_ref - mse_out)/mse_ref\n",
        "\n",
        "  if verbose != 0:\n",
        "    print(f\"mse_in = {mse_in:0.3f}\")\n",
        "    print(f\"mse_out = {mse_out:0.3f}, mse from average y prediction = {mse_ref:0.3f}\")\n",
        "    print(f\"That is improvement of {pct_improvement:0.2f}%\")\n",
        "  return mse_in, mse_out, mse_ref\n",
        "\n",
        "import time\n",
        "\n",
        "begin = time.time()\n",
        "\n",
        "features_to_use = get_best_features(train, TARGET, NUM_FOLDS, NUM_CV_TRIALS)\n",
        "print(f\"Using feartures {features_to_use}.\")\n",
        "model = linear_model.LinearRegression(fit_intercept = True)\n",
        "mse_in, mse_out, mse_ref = final_run_regression(train, test, features_to_use,\n",
        "  model, mean_squared_error, verbose = 1)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Took {end-begin:0.4f} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "xx6CQrJ744C3",
        "outputId": "0c08d486-d4c8-4c69-e5df-a9f9142c2e31"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using feartures ['lcavol', 'lweight', 'svi', 'lbph', 'age', 'pgg45', 'lcp'].\n",
            "mse_in = 0.43936269130473243\n",
            "mse_out = 0.517, mse from average y prediction = 1.057\n",
            "That is improvement of 51.12%\n",
            "Took 103.3749 sec.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "mfXNrU7oKLqW"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "principal_component_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4U3gweQO1lUNEZF5R9sFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sling1678/ML_programs_for_video_lectures/blob/main/principal_component_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Principal Component Analys for dimensionality reduction of $x$\n",
        "\n",
        "Suppose we have $N$ data points of a $p$-dimensional variable $x$,  $\\mathcal{D}=(x_1, x_2, \\cdots x_N)$, where each $x_i$ has $p$ components. For instance $x_1=(x_{11}, x_{12}, \\cdots, x_{1p})$. In this $p$-dimensional space, say $R^p$, we want to find an subspace $R^q$, with $q\\le p$ as \"faithfully\" as possible. By this we mean that if we reconstruct the original points, squared error will be minimum. $\\textbf{This is an unsupervised algorithm.}$\n",
        "\n",
        "Before we get going with calculation, we center our data and introduce matric notation, which will be useful. Let mean of each property in $x$ be\n",
        "\n",
        "$$m_{j} = \\sum_{a=1}^{N} x_{a,j} \\ \\ \\ \\ (\\dagger)$$\n",
        "$$ x^{c}_{i,j} = x_{i,j} - m_{j},\\ \\ j=1,2,\\cdots, p.\\ \\ \\ \\   (1)$$\n",
        "Note that this does not make individual $x^{c}_{i,j}$ equal to zero. \n",
        "To write this in matrix notation, we write all $N$ cases of $x'$ in an $N \\times (p)$ matrix $X^{c}$.\n",
        " \n",
        "$$ x^{c}_{11}\\ x^{c}_{12}\\ \\cdots \\ x^{c}_{1p} $$\n",
        "$$ x^{c}_{21}\\ x^{c}_{22}\\ \\cdots \\ x^{c}_{2p} $$\n",
        "$$\\cdots \\ \\ \\ \\ \\ (Eq. 2)$$\n",
        "$$ x^{c}_{21}\\ x^{c}_{N2}\\ \\cdots \\ x^{c}_{Np} $$\n",
        "\n",
        "$\\textbf{Optimization Problem of PCA}$\n",
        "To map $N$ vectors in $R^p$, whose mean is at $p$-dimensional origin, to $N$ vectors in $R^q$. We use a mapping matrix $V$ with dimensions $p\\times q$ whose columns are $q$ orthonormal vectors. The original vectors $x$ are now $x^c$ due to centering. They map into vectors $\\lambda_i in R^q$. In the following we need to find optimal values of $\\mu \\in R^p$, $\\lambda_i$'s and components of the matrix $V$.\n",
        "\n",
        "<!-- We need to find parameters $\\mu \\in R^p$, $\\lambda_i \\in R^q$, and mapping matrix $V$ with dimensions $p\\times q$ whose columns are $q$ orthonormal vectors, which we will label as $v_1, v_2, ..., v_q$ by  -->\n",
        "\n",
        "$$ {min}_{\\mu,\\lambda_1,..,\\lambda_N, V_{11}, V_{12}, .., V_{pq}} \\sum_{i=1}^{N} \\left( x^c_i - \\mu - V\\lambda_i \\right)^T\\left( x^c_i - \\mu - V\\lambda_i \\right)\\ \\  \\ (3) $$\n",
        "\n",
        "$\\textbf{First Step: Partial Minimization}$\n",
        "\n",
        "First we minimize with respect to $\\mu$ and $\\lambda_i$, while keeping matric $V$ variable.\n",
        "\n",
        "Taking gradient with respect to $\\mu$ immediately yields the optimal $\\mu$ to be\n",
        "\n",
        "$$\\hat\\mu = \\frac{1}{N} \\sum_{i=1}^N x^c_i - \\frac{1}{N} V \\sum_{i=1}^N \\hat \\lambda_i = \\bar x^c - V \\bar \\lambda = - V \\bar \\lambda\\ \\ \\ \\ (4)$$\n",
        "\n",
        "Taking gradient with respect to $\\lambda_j$, for some particular $j = 1, ..., N$ we will get (take derivative with respect to $\\lambda_j^T$) the following for the optimal parameters\n",
        "\n",
        "$$V^TV\\hat\\lambda_j =  V^T\\left( x^c_i - \\hat\\mu\\right)$$\n",
        "\n",
        "Now, using the orthonormally of the the column vector of $V$ gives $V^T V = I$, the $\\q\\times q$ identity. Therefore\n",
        "\n",
        "$$\\hat\\lambda_j =  V^T\\left( x^c_j - \\hat\\mu\\right),\\ j=1,2,,\\cdots, N  \\ \\ \\ (5)$$\n",
        "\n",
        "Notice that if you take the sum of (5) and use (4), this is just an identity. We can choose to set $\\mu$ to zero for simplicity without loss of generality. This choice gives the following for $\\hat \\mu$ and $\\hat \\lambda_i$.\n",
        "\n",
        "$$ \\hat \\mu = 0;\\ \\ \\ \\hat\\lambda_i = V^T x^c_i\\ \\ \\ (6)$$\n",
        "\n",
        "$\\textbf{Step 2: Simplify (3) and optimize with respect to V}:$\n",
        "Using (5) in (3) we convert it to a problem only in V.\n",
        "$$\n",
        "{min}_{V_{11}, V_{12}, .., V_{pq}} \\sum_{i=1}^{N} \\left( x^c_i - VV^Tx^c_i \\right)^T\\left( x^c_i - VV^Tx^x_i \\right)\\ \\  \\ (7)\n",
        "$$\n",
        "The solution to this quartic function of $V_{ab}$ is determined from first $q$ columns of the right singular vector of the centered $X^c$ matrix, consrtucted from the data, given in Eq. (2) above. Let us denote the SVD of $X^c$ by\n",
        "\n",
        "$$X^{c} = U D W^T,$$\n",
        "\n",
        "where $U$ is $(N\\times p)$, $D$ a $p\\times p$ diagonal matrix with values $d_1\\ge d_2\\ge\\cdots\\ge d_p\\ge0$, and $W$ a $q\\times p$ matrix with $p\\ge q$. The solution of (7) is first $q$ columns of $W$.\n",
        "\n",
        "$\\textbf{Solution: For centered data X^{c}$\n",
        "\n",
        "$$ V = \\text{First q columns of }W,$$\n",
        "$$ \\hat\\lambda = \\text{First q columns of }(UD)$$\n",
        "$$ \\text{making } \\hat\\lambda_i = \\sum_{j=1}^{q}u_{ij}d_j.$$\n",
        "Columns of $(UD)$ are called principal components of  $\\textbf{principal}$ $\\textbf{components of }$ $X^{c}$, of which first $q$ columns are the orthonormal directions in the lower dimensional target space that minimiza the reconstruction error. \n",
        "\n",
        "## Implementation - Don't reinvent the wheel.\n",
        "class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', n_oversamples=10, power_iteration_normalizer='auto', random_state=None)\n",
        "\n",
        "## What will happen when you choose first 2 principal components?\n",
        "This will mean we have two orthonormal vector $v_1$ and $v_2$ in $W$, only two values $d_1$, $d_2$ of $D$ in $2\\times 2$ top left corner, and two columns of $U$ will be $N\\times 2$ vector. We can a datapoint $N$ values of $x_i$ ($p$-dim) will now be represented by $N$ values along directions $v_1$ and $v_2$ only, whose components are $u_{i1}d_1$ and $u_{i2}d_2$.\n",
        "\n",
        "$$x_i \\text{ replaced by } \\hat x_i = u_{i1}d_1 v_1 + u_{i2}d_2 v_2$$\n"
      ],
      "metadata": {
        "id": "9D8k4Xv_SJ5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Components of Prostate Cancer Data\n",
        "We will use the prostate cancer data referred to in the Elements of Statistical Learning (ESL). The data has eight-dimensional predictor\n",
        "\n",
        "x_1 = log of cancer volume (lcavol); NUMERICAL\n",
        "\n",
        "x_2 = log of prostate weight (lweight); NUMERICAL\n",
        "\n",
        "x_3 = age (age); NUMERICAL\n",
        "\n",
        "x_4 = log of amount of benign prostatic hyperplasia (lbph); NUMERICAL\n",
        "\n",
        "x_5 = seminal vesicle invasion (svi); INTEGER_CATEGORICAL\n",
        "\n",
        "x_6 = log of capsular penetration (lcp); NUMERICAL\n",
        "\n",
        "x_7 = Gleason score (gleason); INTEGER_CATEGORICAL\n",
        "\n",
        "x_8 = percent of Gleason scores 4 or 5 (pgg45). NUMERICAL"
      ],
      "metadata": {
        "id": "1syJtI1usAmT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KBnNAnjsSJA1"
      },
      "outputs": [],
      "source": [
        "#IMPORTS\n",
        "import sys # for sys.stdout.write()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn import linear_model # This will save time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import make_scorer # to convert metrics into scoring function\n",
        "import sklearn.decomposition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Data"
      ],
      "metadata": {
        "id": "mIWyO4l4teQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe for this project\n",
        "DATA_URL = \"https://hastie.su.domains/ElemStatLearn/datasets/prostate.data\"\n",
        "df = pd.read_csv(DATA_URL, sep='\\t') \n",
        "\n",
        "TARGET = ['lpsa']\n",
        "ALL_FEATURES = ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']\n",
        "TRAINING_SET_SELECTION = ['train']\n",
        "df = df[ALL_FEATURES + TARGET + TRAINING_SET_SELECTION] # drops fictitious columns\n",
        "if False:\n",
        "  print(df.head(3))# check"
      ],
      "metadata": {
        "id": "buSyhhCAtgNt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove extra columns in the data \n",
        "def clean_df(df, dropcols=None):\n",
        "  if dropcols is not None:\n",
        "    for col in dropcols:\n",
        "      if col in df.columns:\n",
        "        df.drop(col, axis=1, inplace=True)\n",
        "  return df  \n",
        "\n",
        "def prepare_train_and_test_sets(df):\n",
        "  # special for this dataset; datapoints to be used in training\n",
        "  # are labeled in a separate calumn with letter 'T'\n",
        "  train_col_name, train_value=\"train\",\"T\"\n",
        "  train = df[df[train_col_name]==train_value].copy()\n",
        "  train.drop(columns=[train_col_name], axis=1, inplace=True)\n",
        "  train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  test = df[df[train_col_name]!=train_value].copy()\n",
        "  test.drop(columns=[train_col_name], axis=1, inplace=True)\n",
        "  test.reset_index(drop=True, inplace=True)\n",
        "  return train, test\n",
        "#-------------------------------------------------------\n",
        "train, test = prepare_train_and_test_sets(df)\n",
        "\n",
        "if False:\n",
        "  print(f\"full dataframe shape:{df.shape}\")\n",
        "  print(f\"train dataframe shape:{train.shape}\")\n",
        "  print(f\"test dataframe shape:{test.shape}\")\n",
        "  print(f\"train dataframe first two rows:\\n{train.head(2)}\")"
      ],
      "metadata": {
        "id": "PEJBjDx0tj0C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing- here only normalizing the train and test datasets based on train\n",
        "def preprocess_params(train, features):\n",
        "  means = train[features].mean()\n",
        "  stds = train[features].std()\n",
        "  return means, stds\n",
        "def preprocess(train, test, features):\n",
        "  means, stds = preprocess_params(train, features)\n",
        "  train[features] = (train[features]-means)/stds\n",
        "  test[features] = (test[features]-means)/stds\n",
        "  return train, test\n",
        "#-------------------------------------------------------\n",
        "train, test = preprocess(train, test, features=ALL_FEATURES)\n",
        "if False:\n",
        "  print(train.describe()) # check that mean and std are properly normalized"
      ],
      "metadata": {
        "id": "foAbop7ktnSC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_list(L, name=\"\", num_digits_for_rounding=2):\n",
        "  print(name+\":\")\n",
        "  sys.stdout.write(\"[\\n\")\n",
        "  for i,value in enumerate(L):\n",
        "    sys.stdout.write(str(np.round(value, num_digits_for_rounding))+\", \")\n",
        "    if (i+1)%10 == 0:\n",
        "      sys.stdout.write(\"\\n\")\n",
        "  sys.stdout.write(\"]\\n\") "
      ],
      "metadata": {
        "id": "ZpDP_bDvtutK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_with_pca(train, test, target=TARGET, features=ALL_FEATURES, \n",
        "                 features_to_drop=None, verbose=0):\n",
        "  train, test = preprocess(train, test, features) #centered data\n",
        "\n",
        "  X_train=train.drop(target, axis=1)\n",
        "  if features_to_drop is not None:\n",
        "    X_train=X_train.drop(features_to_drop, axis=1)\n",
        "    corrs = train.drop(features_to_drop, axis=1).corr()[target].values[:-1].flatten()\n",
        "  else:\n",
        "    corrs = train.corr()[target].values[:-1].flatten()\n",
        "  features = X_train.columns\n",
        "  pca = sklearn.decomposition.PCA()\n",
        "  pca.fit(X_train)\n",
        "\n",
        "  sorted_corrs = sorted(list(zip(features, corrs)), key=lambda x: x[1], reverse=True) \n",
        "\n",
        "  if verbose!=0:\n",
        "    print(\"Correlation:\")\n",
        "    for name, corr in sorted_corrs:\n",
        "      print(f\"{name} has corr {corr:0.2f} with {TARGET[0]}\")\n",
        "    print(\"PCA:\")\n",
        "    for i,evr in enumerate(pca.explained_variance_ratio_):\n",
        "      print(f\"v({i}) explains {evr*100:0.2f}%\")\n",
        " \n",
        "    print(pca.feature_names_in_)\n",
        "    print(pca.components_)\n",
        "  return sorted_corrs, pca\n",
        "if True:\n",
        "  corrs, pca = play_with_pca(train, test, target=TARGET, features=ALL_FEATURES, \n",
        "                 features_to_drop=None, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVsBMBb-uD4f",
        "outputId": "decabcfa-e4e1-4e1e-eab2-5a3fe270db8e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation:\n",
            "lcavol has corr 0.73 with lpsa\n",
            "svi has corr 0.56 with lpsa\n",
            "lcp has corr 0.49 with lpsa\n",
            "lweight has corr 0.49 with lpsa\n",
            "pgg45 has corr 0.45 with lpsa\n",
            "gleason has corr 0.34 with lpsa\n",
            "lbph has corr 0.26 with lpsa\n",
            "age has corr 0.23 with lpsa\n",
            "PCA:\n",
            "v(0) explains 42.83%\n",
            "v(1) explains 20.41%\n",
            "v(2) explains 12.96%\n",
            "v(3) explains 7.73%\n",
            "v(4) explains 5.69%\n",
            "v(5) explains 4.71%\n",
            "v(6) explains 3.50%\n",
            "v(7) explains 2.18%\n",
            "['lcavol' 'lweight' 'age' 'lbph' 'svi' 'lcp' 'gleason' 'pgg45']\n",
            "[[ 0.43599586  0.16758039  0.24092144  0.03036091  0.39573233  0.45997707\n",
            "   0.39453063  0.44611892]\n",
            " [ 0.03227524  0.56469378  0.41986493  0.65026307 -0.17548575 -0.1700181\n",
            "  -0.05512594 -0.13494571]\n",
            " [-0.28475094 -0.38999586  0.3663428   0.06191023 -0.42152807 -0.2063685\n",
            "   0.55309889  0.32029355]\n",
            " [-0.05461772  0.01187868 -0.75335526  0.53918032 -0.13978894  0.10108778\n",
            "   0.19665428  0.26492196]\n",
            " [-0.39775985  0.69191395 -0.15179842 -0.46221891 -0.07040551 -0.12225376\n",
            "   0.17762142  0.27368014]\n",
            " [ 0.6358879   0.13434141 -0.12063335 -0.26049912 -0.67246762  0.04927275\n",
            "   0.12735987 -0.15596705]\n",
            " [ 0.28247165  0.01918199 -0.16074899 -0.01095251  0.3969866  -0.68455517\n",
            "   0.45566894 -0.24501693]\n",
            " [-0.28712919  0.06032735 -0.02199332  0.00786038  0.02461331  0.46980706\n",
            "   0.4912345  -0.67136884]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## At this point you would choose a hyperparameter num_principal_components by a series of experiments and model selection. Suppose you want to use these principal components as your new x variables. You will then pca.transform(X_train) and pca.transform(test). You will then conduct n=1, 2, 3, .., num_features to select various number of principal components. Then, separate train/validation set. Call pca.fit(X_train) and fit.transform(X_train) and fit.transform(X_val). Then, use new X_train and X_val to train your lasso or whatever model and evaluate them with X_val. Plot metric versus n. See if you can spot the optimum n. \n",
        "\n",
        "#I am going to move on. Perhaps come back another day."
      ],
      "metadata": {
        "id": "uH3pDCt20AnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use corr to remove anything whose correlation with target is less than 0.2\n"
      ],
      "metadata": {
        "id": "9sSiSG1o8A72"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g7sJenrT2HEi"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}